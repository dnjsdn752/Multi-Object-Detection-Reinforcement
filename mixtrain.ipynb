{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\coding\\Pytorch_Deep_RL_1-master\\Pytorch_Deep_RL_1-master\\reinforcement.py:84: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from image_helper import *\n",
    "from parse_xml_annotations import *\n",
    "from features import *\n",
    "from reinforcement import *\n",
    "from metrics import *\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "path_voc = \"C:/startcoding/Active-Object-Localization-Deep-Reinforcement-Learning-master/Active-Object-Localization-Deep-Reinforcement-Learning-master/datasets/VOCdevkit/VOC2007\"\n",
    "\n",
    "# get models \n",
    "print(\"load models\")\n",
    "\n",
    "model_vgg = getVGG_16bn(\"../models\")\n",
    "model_vgg = model_vgg.cuda()\n",
    "model = get_q_network()\n",
    "model = model.cuda()\n",
    "\n",
    "# define optimizers for each model\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-6)\n",
    "criterion = nn.MSELoss().cuda()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load imagesC:/startcoding/Active-Object-Localization-Deep-Reinforcement-Learning-master/Active-Object-Localization-Deep-Reinforcement-Learning-master/datasets/VOCdevkit/VOC2007\n",
      "total image:421\n",
      "load imagesC:/startcoding/Active-Object-Localization-Deep-Reinforcement-Learning-master/Active-Object-Localization-Deep-Reinforcement-Learning-master/datasets/VOCdevkit/VOC2012\n",
      "total image:1286\n",
      "aeroplane_trainval image:1707\n"
     ]
    }
   ],
   "source": [
    "# get image datas\n",
    "path_voc_1 = \"C:/startcoding/Active-Object-Localization-Deep-Reinforcement-Learning-master/Active-Object-Localization-Deep-Reinforcement-Learning-master/datasets/VOCdevkit/VOC2007\"\n",
    "path_voc_2 = \"C:/startcoding/Active-Object-Localization-Deep-Reinforcement-Learning-master/Active-Object-Localization-Deep-Reinforcement-Learning-master/datasets/VOCdevkit/VOC2012\"\n",
    "class_object = 'dog' #aeroplane(1) dog(12)\n",
    "image_names_1, images_1 = load_image_data(path_voc_1, class_object)\n",
    "image_names_2, images_2 = load_image_data(path_voc_2, class_object)\n",
    "image_names = image_names_1 + image_names_2\n",
    "images = images_1 + images_2\n",
    "\n",
    "print(\"aeroplane_trainval image:%d\" % len(image_names))\n",
    "\n",
    "# define the Pytorch Tensor\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the super parameter\n",
    "epsilon = 1.0\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.90\n",
    "CLASS_OBJECT = 1\n",
    "steps = 14\n",
    "change = 4\n",
    "epochs = 50\n",
    "alpha = 0.2\n",
    "memory = ReplayMemory(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_h_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        action = np.random.randint(1,7)\n",
    "    else:\n",
    "        qval = model(Variable(state))\n",
    "        _, predicted = torch.max(qval.data,1)\n",
    "        action = predicted[0] + 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_d_action(self, state, actions, ground_truth):\n",
    "        \"\"\"\n",
    "            Selection de l'action dépendemment de l'état\n",
    "            상태에 따른 동작 선택\n",
    "            Entrée :\n",
    "                - Etat actuel. \n",
    "                - Vérité terrain.\n",
    "            Sortie :\n",
    "                - Soi l'action qu'aura choisi le modèle soi la meilleure action possible ( Le choix entre les deux se fait selon un jet aléatoire ).\n",
    "        \"\"\"\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    inpu = Variable(state).cuda()\n",
    "                else:\n",
    "                    inpu = Variable(state)\n",
    "                qval = self.policy_net(inpu)\n",
    "                _, predicted = torch.max(qval.data,1)\n",
    "                action = predicted[0] # + 1\n",
    "                try:\n",
    "                  return action.cpu().numpy()[0]\n",
    "                except:\n",
    "                  return action.cpu().numpy()\n",
    "        else:\n",
    "            #return np.random.randint(0,9)   # Avant implémentation d'agent expert\n",
    "            return get_best_next_action(actions, ground_truth) # Appel à l'agent expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_next_action(self, actions, ground_truth):\n",
    "        \"\"\"\n",
    "            Implémentation de l'Agent expert qui selon l'état actuel et la vérité terrain va donner la meilleur action possible.\n",
    "            현재 상태와 실제 상황에 따라 가능한 최선의 조치를 제공하는 Expert Agent 구현.\n",
    "            Entrée :\n",
    "                - Liste d'actions executées jusqu'à présent.\n",
    "                - Vérité terrain.\n",
    "            Sortie :\n",
    "                - Indice de la meilleure action possible.\n",
    "\n",
    "        \"\"\"\n",
    "        max_reward = -99\n",
    "        best_action = -99\n",
    "        positive_actions = []\n",
    "        negative_actions = []\n",
    "        actual_equivalent_coord = self.calculate_position_box(actions)\n",
    "        for i in range(0, 9):\n",
    "            copy_actions = actions.copy()\n",
    "            copy_actions.append(i)\n",
    "            new_equivalent_coord = self.calculate_position_box(copy_actions)\n",
    "            if i!=0:\n",
    "                reward = self.compute_reward(new_equivalent_coord, actual_equivalent_coord, ground_truth)\n",
    "            else:\n",
    "                reward = self.compute_trigger_reward(new_equivalent_coord,  ground_truth)\n",
    "            \n",
    "            if reward>=0:\n",
    "                positive_actions.append(i)\n",
    "            else:\n",
    "                negative_actions.append(i)\n",
    "        if len(positive_actions)==0:\n",
    "            return random.choice(negative_actions)\n",
    "        return random.choice(positive_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_position_box(self, actions, xmin=0, xmax=224, ymin=0, ymax=224):\n",
    "        \"\"\"\n",
    "            Prends l'ensemble des actions depuis le début et en génére les coordonnées finales de la boite englobante.\n",
    "            처음부터 모든 작업을 수행하고 경계 상자의 최종 좌표를 생성합니다.\n",
    "            Entrée :\n",
    "                - Ensemble des actions sélectionnées depuis le début.\n",
    "            Sortie :\n",
    "                - Coordonnées finales de la boite englobante.\n",
    "        \"\"\"\n",
    "        # Calcul des alpha_h et alpha_w mentionnées dans le papier\n",
    "        alpha_h = self.alpha * (  ymax - ymin )\n",
    "        alpha_w = self.alpha * (  xmax - xmin )\n",
    "        real_x_min, real_x_max, real_y_min, real_y_max = 0, 224, 0, 224\n",
    "\n",
    "        # Boucle sur l'ensemble des actions\n",
    "        for r in actions:\n",
    "            if r == 1: # Right\n",
    "                real_x_min += alpha_w\n",
    "                real_x_max += alpha_w\n",
    "            if r == 2: # Left\n",
    "                real_x_min -= alpha_w\n",
    "                real_x_max -= alpha_w\n",
    "            if r == 3: # Up \n",
    "                real_y_min -= alpha_h\n",
    "                real_y_max -= alpha_h\n",
    "            if r == 4: # Down\n",
    "                real_y_min += alpha_h\n",
    "                real_y_max += alpha_h\n",
    "            if r == 5: # Bigger\n",
    "                real_y_min -= alpha_h\n",
    "                real_y_max += alpha_h\n",
    "                real_x_min -= alpha_w\n",
    "                real_x_max += alpha_w\n",
    "            if r == 6: # Smaller\n",
    "                real_y_min += alpha_h\n",
    "                real_y_max -= alpha_h\n",
    "                real_x_min += alpha_w\n",
    "                real_x_max -= alpha_w\n",
    "            if r == 7: # Fatter\n",
    "                real_y_min += alpha_h\n",
    "                real_y_max -= alpha_h\n",
    "            if r == 8: # Taller\n",
    "                real_x_min += alpha_w\n",
    "                real_x_max -= alpha_w\n",
    "        real_x_min, real_x_max, real_y_min, real_y_max = self.rewrap(real_x_min), self.rewrap(real_x_max), self.rewrap(real_y_min), self.rewrap(real_y_max)\n",
    "        return [real_x_min, real_x_max, real_y_min, real_y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "def optimizer_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "    next_states = [s for s in batch.next_state if s is not None]\n",
    "    non_final_next_states = Variable(torch.cat(next_states), \n",
    "                                     volatile=True).type(Tensor)\n",
    "    state_batch = Variable(torch.cat(batch.state)).type(Tensor)\n",
    "    action_batch = Variable(torch.LongTensor(batch.action).view(-1,1)).type(LongTensor)\n",
    "    reward_batch = Variable(torch.FloatTensor(batch.reward).view(-1,1)).type(Tensor)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE, 1).type(Tensor)) \n",
    "    next_state_values[non_final_mask.view(-1,1)] = model(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    # Now, we don't want to mess up the loss with a volatile flag, so let's\n",
    "    # clear it. After this, we'll just end up with a Variable that has\n",
    "    # requires_grad=False\n",
    "    next_state_values.volatile = False\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Compute  loss\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_state(self, image, dtype=FloatTensor):\n",
    "        \"\"\"\n",
    "            Composition d'un état : Feature Vector + Historique des actions\n",
    "            상태 구성: 기능 벡터 + 작업 내역\n",
    "            Entrée :\n",
    "                - Image ( feature vector ). \n",
    "            Sortie :\n",
    "                - Représentation d'état.\n",
    "        \"\"\"\n",
    "        image_feature = get_features(image, dtype)\n",
    "        image_feature = image_feature.view(1,-1)\n",
    "        #print(\"image feature : \"+str(image_feature.shape))\n",
    "        history_flatten = self.actions_history.view(1,-1).type(dtype)\n",
    "        state = torch.cat((image_feature, history_flatten), 1)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(self, image, dtype=FloatTensor):\n",
    "        \"\"\"\n",
    "            Extraction du feature vector à partir de l'image.\n",
    "            이미지에서 특징 벡터를 추출합니다.\n",
    "            Entrée :\n",
    "                - Image\n",
    "            Sortie :\n",
    "                - Feature vector\n",
    "        \"\"\"\n",
    "    \n",
    "        global transform\n",
    "        #image = transform(image)\n",
    "        image = image.view(1,*image.shape)\n",
    "        image = Variable(image).type(dtype)\n",
    "        if use_cuda:\n",
    "            image = image.cuda()\n",
    "        feature = self.feature_extractor(image)\n",
    "        #print(\"Feature shape : \"+str(feature.shape))\n",
    "        return feature.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nactive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
